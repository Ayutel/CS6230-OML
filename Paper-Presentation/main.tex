\documentclass{beamer}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bookmark}
\usepackage{hyperref}
\newcommand{\h}{\nabla^{2}}
\newcommand{\g}{\nabla}
\newcommand{\xbold}{\mathbf{x}}
%\documentclass{article}
%\usepackage{beamerarticle}
\usetheme{CambridgeUS}

\title[Xu, Roosta-Khorasani and Mahoney]{Newton-Type Methods for Non-Convex Optimization Under Inexact Hessian Information}
\subtitle{Presented as a part of\\
CS6230 - Optimization Methods in Machine Learning}

\author{Vishwak Srinivasan \and Ayushi Patel}

% Let's get started
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
  % You might wish to add the option [pausesections]
\end{frame}

% Section and subsections will appear in the presentation overview
% and table of contents.
\section{Introduction}
\subsection{The problem statement}

\begin{frame}{The problem statement}
  \begin{itemize}
  \item {
    Consider a unconstrained optimization problem
    \begin{equation}
        \label{main-obj}
        \min_{\xbold \in \mathbb{R}^{d}} F(\xbold)
    \end{equation}
    where \(F : \mathbb{R}^{d} \rightarrow \mathbb{R}\), is \textit{smooth} and \textit{non-convex}.
  }
  \item<2-> {
    Many known methods to help solve the \emph{convex} version.
    \begin{itemize}
        \item First order methods
        \item Second order methods
    \end{itemize}
  }
  \item<3-> {
    First order methods are ``fine" - advances in tensor computing, automatic differentiation and so on.
    \begin{equation}
        \xbold := \xbold - \eta \g F(\xbold)
    \end{equation}
    \pause
  }
  \item<4-> {
    Are second order methods just as fine?
    \begin{equation}
        \xbold := \xbold - \eta (\h F(\xbold))^{-1} \g F(\xbold)
    \end{equation}
    \uncover<5->{How do we compute \(\h F(\xbold)\)?}
  }
  \end{itemize}
\end{frame}

\subsection{Motivation for the work}
\begin{frame}{Issues with the first order methods}
  \begin{itemize}
  \item<1->{
    Performance of first order methods can be seriously hindered by ill-conditioning.
    \begin{itemize}    
        \item<2->{Condition number: \(\kappa = \frac{L}{\gamma} = \frac{\lambda_{\max}(\h F)}{\lambda_{\min}(\h F)}\), where \(L\) is smoothness parameter, \(\gamma\) is strong convexity parameter}
        \item<3->{Convergence criteria for gradient descent relies on \(\kappa\)} %add citations
    \end{itemize}
  }
  \item<4->{
    First order methods involve fine-tuning hyperparameters.
    \begin{itemize}    
        \item<5->{For gradient descent, we have 1 (learning rate) and for Adam we have 4 (learning rate, tolerance, \(\beta_{1}\) and \(\beta_{2}\))}
    \end{itemize}
  }
  \item<6->{
    Theoretical guarantees exist for \emph{convex} settings only.
    \begin{itemize}    
        \item<7->{Work by Nesterov showed that we take \(O(1/\epsilon)\) iterations to converge to a first order stationary point using gradient descent} % add citations
        \item<8->{\emph{Non-convex} problems have saddle-points to escape from too}
    \end{itemize}
  }
  \end{itemize}
\end{frame}

\begin{frame}{Issues with the second order methods}
  \begin{itemize}
  \item<1-> {
    Second order methods: computation of \(\h F(\xbold)\)
    \begin{itemize}
      \uncover<2->{\item How hard is this? Time Complexity Analysis}
      \uncover<3->{\item How hard is this? Space Complexity Analysis}
    \end{itemize}
  }
  \item<4->{
    Theoretical guarantees exist for \emph{convex} settings only.
  } %add citations here as well
  \end{itemize}
\end{frame}

\section{Definitions}
\begin{frame}{Special Notations}
\begin{itemize}
\item<1->{\(\g F(\xbold)\) is the gradient of \(F\) at \(\xbold\)}
\item<2->{\(\h F(\xbold)\) is the (Exact) Hessian of \(F\) at \(\xbold \implies\) \(F\) is twice-differentiable}
\item<3->{\(H_{t} \triangleq H(\xbold_{t})\) denotes the In-exact Hessian of \(F\) at the \(t^{th}\) iteration or at \(\xbold_{t}\)}
\item<4->{\(A \succ B\) for two symmetric matrices \(A, B\) implies \(A - B \succ 0\). Similar notation holds for positive-semidefiniteness}
\end{itemize}
\end{frame}

\begin{frame}{Definitions}
\begin{alertblock}{\((\epsilon_{g}, \epsilon_{h})\)-Optimality}
Given \(\epsilon_{g}, \epsilon_{h} \in (0, 1)\), \(\mathbf{\hat{x}}\) is an \((\epsilon_{g}, \epsilon_{h})\)-optimal solution to the problem \ref{main-obj}, if:
\begin{itemize}
\item \(||\g F(\mathbf{\hat{x}}) || \leq \epsilon_{g}\)
\item \(\lambda_{\min} (\h F(\mathbf{\hat{x}})) \leq \epsilon_{h}\)
\end{itemize}
\end{alertblock}
\pause
Note: If the function \(F\) satisfies strict saddle property, then the above defined optimality condition ensures ``closeness'' to a local-minimum. % add citations
\pause
\begin{alertblock}{Lipschitz and Bounded Hessian (Hessian Regularity)}
Given \(\xbold_{t}\) and \(\mathbf{s}_{t}\) the \(t^{th}\) iterate and \(t^{th}\) update step:
\begin{itemize}
\item \(||\h F(\xbold) - \h F(\xbold_{t})|| \leq L||\xbold - \xbold_{t}|| \hspace{4mm} \forall \xbold \in [\xbold_{t}, \xbold_{t} + \mathbf{s}_{t}]\)
\item \(||\h F(\xbold_{t}) || \leq B\)
\end{itemize}
\end{alertblock}
\end{frame}
\end{document}
