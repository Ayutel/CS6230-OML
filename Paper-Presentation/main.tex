\documentclass{beamer}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bookmark}
\usepackage{hyperref}
\newcommand{\h}{\nabla^{2}}
\newcommand{\g}{\nabla}
\newcommand{\xbold}{\mathbf{x}}
%\documentclass{article}
%\usepackage{beamerarticle}
\usetheme{CambridgeUS}

\title[Xu, Roosta-Khorasani and Mahoney]{Newton-Type Methods for Non-Convex Optimization Under Inexact Hessian Information}
\subtitle{Presented as a part of\\
CS6230 - Optimization Methods in Machine Learning}

\author{Vishwak Srinivasan \and Ayushi Patel}

% Let's get started
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
  % You might wish to add the option [pausesections]
\end{frame}

% Section and subsections will appear in the presentation overview
% and table of contents.
\section{Introduction}
\subsection{The problem statement}

\begin{frame}{The problem statement}
  \begin{itemize}
  \item {
    Consider a unconstrained optimization problem
    \begin{equation}
        \label{main-obj}
        \min_{\xbold \in \mathbb{R}^{d}} F(\xbold)
    \end{equation}
    where \(F : \mathbb{R}^{d} \rightarrow \mathbb{R}\), is \textit{smooth} and \textit{non-convex}.
  }
  \item<2-> {
    Many known methods to help solve the \emph{convex} version.
    \begin{itemize}
        \item First order methods
        \item Second order methods
    \end{itemize}
  }
  \item<3-> {
    First order methods are ``fine" - advances in tensor computing, automatic differentiation and so on.
    \begin{equation}
        \xbold := \xbold - \eta \g F(\xbold)
    \end{equation}
    \pause
  }
  \item<4-> {
    Are second order methods just as fine?
    \begin{equation}
        \xbold := \xbold - \eta (\h F(\xbold))^{-1} \g F(\xbold)
    \end{equation}
    \uncover<5->{How do we compute \(\h F(\xbold)\)?}
  }
  \end{itemize}
\end{frame}

\subsection{Motivation for the work}
\begin{frame}{Issues with the first order methods}
  \begin{itemize}
  \item<1->{
    Performance of first order methods can be seriously hindered by ill-conditioning.
    \begin{itemize}    
        \item<2->{Condition number: \(\kappa = \frac{L}{\gamma} = \frac{\lambda_{\max}(\h F)}{\lambda_{\min}(\h F)}\), where \(L\) is smoothness parameter, \(\gamma\) is strong convexity parameter}
        \item<3->{Convergence criteria for gradient descent relies on \(\kappa\)} %add citations
    \end{itemize}
  }
  \item<4->{
    First order methods involve fine-tuning hyperparameters.
    \begin{itemize}    
        \item<5->{For gradient descent, we have 1 (learning rate) and for Adam we have 4 (learning rate, tolerance, \(\beta_{1}\) and \(\beta_{2}\))}
    \end{itemize}
  }
  \item<6->{
    Theoretical guarantees exist for \emph{convex} settings only.
    \begin{itemize}    
        \item<7->{\emph{Non-convex} problems have saddle-points to escape from}
    \end{itemize}
  }
  \end{itemize}
\end{frame}

\begin{frame}{Issues with the second order methods}
  \begin{itemize}
  \item<1-> {
    Second order methods: computation of \(\h F(\xbold)\)
    \begin{itemize}
      \uncover<2->{\item How hard is this? Time Complexity Analysis}
      \uncover<3->{\item How hard is this? Space Complexity Analysis}
    \end{itemize}
  }
  \item<4->{
    Theoretical guarantees exist for \emph{convex} settings only.
  } %add citations here as well
  \end{itemize}
\end{frame}

\section{Notations, Definitions and Preliminaries}
\begin{frame}{Special Notations}
\begin{itemize}
\item<1->{\(\g F(\xbold)\) is the gradient of \(F\) at \(\xbold\)}
\item<2->{\(\h F(\xbold)\) is the (Exact) Hessian of \(F\) at \(\xbold \implies\) \(F\) is twice-differentiable}
\item<3->{\(H_{t} \triangleq H(\xbold_{t})\) denotes the In-exact Hessian of \(F\) at the \(t^{th}\) iteration or at \(\xbold_{t}\)}
\item<4->{\(A \succ B\) for two symmetric matrices \(A, B\) implies \(A - B \succ 0\). Similar notation holds for positive-semidefiniteness}
\end{itemize}
\end{frame}

\begin{frame}{Definitions}
\begin{alertblock}{\((\epsilon_{g}, \epsilon_{h})\)-Optimality}
\label{sec-opt}
Given \(\epsilon_{g}, \epsilon_{h} \in (0, 1)\), \(\mathbf{\hat{x}}\) is an \((\epsilon_{g}, \epsilon_{h})\)-optimal solution to the problem \ref{main-obj}, if:
\begin{itemize}
\item \(||\g F(\mathbf{\hat{x}}) || \leq \epsilon_{g}\)
\item \(\lambda_{\min} (\h F(\mathbf{\hat{x}})) \geq -\epsilon_{h}\)
\end{itemize}
\end{alertblock}
\pause
Note: If the function \(F\) satisfies \underline{strict saddle property}, then the above defined optimality condition ensures ``closeness'' to a local-minimum. % add citations
\pause
\begin{alertblock}{Lipschitz and Bounded Hessian (Hessian Regularity)}
Given \(\xbold_{t}\) and \(\mathbf{s}_{t}\) the \(t^{th}\) iterate and \(t^{th}\) update step:
\begin{itemize}
\item \(||\h F(\xbold) - \h F(\xbold_{t})|| \leq L||\xbold - \xbold_{t}|| \hspace{4mm} \forall \xbold \in [\xbold_{t}, \xbold_{t} + \mathbf{s}_{t}]\)
\item \(||\h F(\xbold_{t}) || \leq B\)
\end{itemize}
\end{alertblock}
\end{frame}

\subsection{Preliminaries}
\begin{frame}{Trust Region Methods}
\begin{itemize}
\item<1->{Method used to optimize over \emph{non-convex} functions, proven to be extremely useful at or near saddle points, by leveraging the negative curvature around that point}
\item<2->{Helps achieve two things:
  \begin{itemize}
    \item<3->Finds optimal direction to move in
    \item<4->Finds optimal distance to move in the determined direction
  \end{itemize}
  }
\item<5->{Mathematically: if \(\mathbf{s}_{t}\) is the update to be made i.e., \(\xbold_{t+1} - \xbold_{t} = \mathbf{s}_{t}\), then
\begin{equation}
\label{TR}
\mathbf{s}_{t} = \text{arg}\min_{\mathbf{s}} \frac{1}{2}\mathbf{s}^{T} \h F(\xbold_{t}) \mathbf{s} + \mathbf{s}^{T} \g F(\xbold_{t}) \hspace{5mm}\ni ||\mathbf{s}||_{2} \leq \Delta_{t}
\end{equation} % add citation
}
\end{itemize}
\end{frame}

\begin{frame}{Trust Region Methods: Theoretical Guarantees}
\begin{itemize}
\item<1->{Solve a minimization problem which is a constrained quadratic problem}
\item<2->{Similarities to other eigenvalue problems, leading to special methods drawing inferences from existing methods e.g., truncated conjugate gradient methods, Lanczos trust-region based methods}
\item<3->{To reach an \((\epsilon_{g}, \epsilon_{h})\)-optimal point as defined earlier, we need \(O\left(\max\{\epsilon_{h}^{-1}\epsilon_{g}^{-2}, \epsilon_{h}^{-3}\}\right)\)} %add citation
\item<4->{To reach an \((\epsilon_{g}, \epsilon_{h})\)-optimal point, we need \(O\left(\max\{\epsilon_{g}^{-3}, \epsilon_{h}^{-3}\}\right)\)} %add citation
\end{itemize}
\end{frame}

\begin{frame}{Cubic Regularization Methods}
\begin{itemize}
\item<1->{Method used to optimize over \emph{non-convex} functions, just as done by trust region methods.}
\item<2->{Provides the same benefits provided by trust region methods
  \begin{itemize}
    \item<3->Finds optimal direction to move in
    \item<4->Finds optimal distance to move in the determined direction
  \end{itemize}
  }
\item<5->{Mathematically: if \(\mathbf{s}_{t}\) is the update to be made i.e., \(\xbold_{t+1} - \xbold_{t} = \mathbf{s}_{t}\), then
\begin{equation}
\label{CR}
\mathbf{s}_{t} = \text{arg}\min_{\mathbf{s}} \frac{1}{2}\mathbf{s}^{T} \h F(\xbold_{t}) \mathbf{s} + \mathbf{s}^{T} \g F(\xbold_{t}) + \frac{\sigma_{t}}{3}||\mathbf{s}||^3
\end{equation} % add citation
}
\item<6->{Notice familiarity?}
\item<7->{What is different?}
\end{itemize}
\end{frame}

\begin{frame}{Cubic Regularization Methods: Theoretical Guarantess}
\begin{itemize}
\item<1->{Solve a minimization problem which is an unconstrained quadratic problem, as opposed to constrained objective in \ref{TR}}
\item<2->{Methods to solve this include Lanczos type iterations, fast approximate matrix inversion}
\item<3->{To reach an \((\epsilon_{g}, \epsilon_{h})\)-optimal point as defined earlier, we need \(O\left(\max\{\epsilon_{g}^{-2}, \epsilon_{h}^{-3}\}\right)\)} %add citation
\item<4->{Assuming more tight conditions helps improve the dependence on \(\epsilon_{g}\) - we need \(O\left(\max\{\epsilon_{g}^{-3/2}, \epsilon_{h}^{-3}\}\right)\)} %add citation
\end{itemize}
\end{frame}
\end{document}
