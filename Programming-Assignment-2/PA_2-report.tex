\documentclass{article}
\usepackage[margin=0.73in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{bookmark}
\usepackage{hyperref}
\newcommand{\xtilde}{\widetilde{X}}
\newcommand{\indi}{\mathbb{I}}
\newcommand{\setS}{\mathcal{S}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\lagL}{\mathcal{L}}
\newcommand{\minimize}{\mathop{\mathrm{minimize}}}
\newcommand{\maximize}{\mathop{\mathrm{maximize}}}
\newcommand{\minimizewrt}[1]{\underset{#1}{\minimize}}   
\newcommand{\maximizewrt}[1]{\underset{#1}{\maximize}}   

\title{Report for Programming Assignment 2}
\author{Vishwak Srinivasan\\
\texttt{CS15BTECH11043}}
\date{}

\begin{document}
\maketitle

\section*{Instructions for the code}
\begin{itemize}
\item There are two files: \texttt{primal.py} and \texttt{dual.py}. \texttt{primal.py} concerns questions b.1, b.2 and b.4. \texttt{dual.py} concerns question b.3.
\item The programs used \texttt{CVXPY}, a convex programming module in Python.
\item To run these programs, enter \texttt{python3 <file>.py --C1 <C1-value> --C2 <C2-value>}. \texttt{<file>} can be any one of \texttt{primal} and \texttt{dual}.
\item The output of \texttt{dual.py} requires the output of \texttt{primal.py} for the same parameters \texttt{C1} and \texttt{C2}. So please run \texttt{primal.py} first, and then \texttt{dual.py}. \texttt{primal.py} writes a \texttt{beta.txt} file that is used by \texttt{dual.py}, which also uses a \texttt{alpha.txt} file.
\end{itemize}

\section*{Part a}
\subsection*{Question 1}
\begin{flushleft}
Strong duality holds for the below problem:
\begin{multline}
\label{primal}
\minimizewrt{\beta \in \real^{p}, \hspace{0.5mm} \beta_{0} \in \real, \hspace{0.5mm} \xi \in \real^{n}} \frac{1}{2}||\beta||_{2}^{2} + C_{1}\sum_{i \in \setS_{1}} \xi_{i} + C_{2}\sum_{i \in \setS_{2}} \xi_{i} \\
\text{subject to } \xi_{i} \geq 0, \hspace{2mm} y_{i}(x_{i}^{T}\beta + \beta_{0}) \geq 1 - \xi_{i}, \hspace{2mm} \forall i \in \{1, 2, \ldots, n\}
\end{multline}

The reasons are:
\begin{itemize}
\item The objective function is convex.
\item The constraints are affine in the variables to be optimized over.
\item \(\xi = \mathbf{1}, \beta = \mathbf{0}, \beta_{0} = 0\) is a point of strict feasibility in constraints.
\item Using the above three points and Slater's condition, we can tell \ref{primal} has strong duality.
\end{itemize}
\end{flushleft}

\subsection*{Question 2}
\begin{flushleft}
Define the Lagrangian \(\mathcal{L}\) as:
\begin{equation}
\label{lagrangian}
\displaystyle \lagL(\beta, \beta_{0}, \xi, \alpha, \mu) = \frac{1}{2}||\beta||_{2}^{2} + C_{1}\sum_{i \in \setS_{1}}\xi_{i} + C_{2}\sum_{i \in \setS_{2}}\xi_{i} + \sum_{i=1}^{n}\alpha_{i}(1 - \xi_{i} - y_{i}(x_{i}^{T}\beta + \beta_{0})) - \sum_{i=1}^{n}\mu_{i}(\xi_{i})
\end{equation}

By property of stationarity in \ref{lagrangian}, assuming \(\beta, \beta_{0}, \xi\) are solutions to the primal and \(\alpha, \mu\) are the solutions to the dual and using the stationarity sub-condition of the KKT conditions, we get:
\begin{gather}
\label{stationarity-beta}
\displaystyle \nabla_{\beta} \lagL = \beta + \sum_{i=1}^{n}\alpha_{i}(-y_{i}x_{i}) = \mathbf{0} \implies \beta = \sum_{i=1}^{n}\alpha_{i}y_{i}x_{i} \\
\label{stationarity-beta-0}
\displaystyle \frac{\partial \lagL}{\partial \beta_{0}} = -\sum_{i=1}^{n}\alpha_{i}y_{i} = 0 \implies \sum_{i=1}^{n}\alpha_{i}y_{i} = 0 \\
\label{stationarity-xi}
\displaystyle \frac{\partial \lagL}{\partial \xi_{i}} = C_{1}\indi_{i \in \setS_{1}} + C_{2}\indi_{i \in \setS_{2}} - \alpha_{i} - \mu_{i} = 0 \implies \alpha_{i} + \mu_{i} = C_{j} \hspace{1.5mm} \forall i \in \setS_{j} \hspace{1.5mm} \forall j \in \{1, 2\}
\end{gather}

Now using the complementary slackness sub-condition of the KKT conditions, we get:
\begin{gather}
\label{compl-slack-alpha}
\displaystyle \alpha_{i}(1 - \xi_{i} + y_{i}(x_{i}^{T}\beta + \beta_{0})) = 0 \hspace{4mm} \forall i \in \{1, 2, \ldots, n\} \\
\label{compl-slack-mu}
\displaystyle \mu_{i}\xi_{i} = 0 \hspace{4mm} \forall i \in \{1, 2, \ldots, n\}
\end{gather}

Using primal feasibility:
\begin{equation}
\label{primal-fease}
\xi_{i}, y_{i}(\beta^{T}x_{i} + \beta_{0}) \geq 0 \hspace{4mm} \forall i \in \{1, 2, \ldots, n\}
\end{equation}

Using dual feasibility:
\begin{equation}
\label{dual-fease}
\alpha_{i}, \mu_{i} \geq 0 \hspace{4mm} \forall i \in \{1, 2, \ldots, n\}
\end{equation}
\end{flushleft}

\subsection*{Question 3}
\begin{flushleft}
Note that \ref{stationarity-beta-0} can be written as \(y^{T}\alpha = 0\) by definition of the inner product. Similarly, construct a matrix:
\begin{equation}
\label{y-diag}
\mathop{diag}(y) = \begin{bmatrix} y_{1} & 0 & \ldots & 0 \\ 0 & y_{2} & & \vdots \\ \vdots & & \ddots & 0 \\ 0 & \ldots & 0 & y_{n} \end{bmatrix} 
\end{equation}

Using \ref{y-diag}, we can note that:
\begin{equation}
\label{x-tilde}
\xtilde = \mathop{diag}(y)X = \begin{bmatrix} y_{1}x_{1} \\ \vdots \\ y_{n}x_{n} \end{bmatrix}
\end{equation}

From \ref{stationarity-beta}, we can rewrite \(\beta\) as:
\begin{equation}
\label{beta-x-tilde}
\beta = \sum_{i=1}^{n} \alpha_{i}\xtilde_{i} = \xtilde^{T}\alpha \implies ||\beta||_{2}^{2} = \beta^{T}\beta = \alpha^{T}\xtilde\xtilde^{T}\alpha
\end{equation}

Using \ref{dual-fease} and \ref{stationarity-xi}, we can tell that:
\begin{equation}
\label{dual-constraints}
0 \leq \alpha_{i} \leq C_{1}\indi_{i \in \setS_{1}} + C_{2}\indi_{i \in \setS_{2}} \implies \mathbf{0} \leq \alpha_{\setS_{j}} \leq C_{j}\mathbf{1} \hspace{2mm} \forall j \in \{1, 2\}
\end{equation}

The above step in \ref{dual-constraints} uses the fact that if \(i \in \setS_{1}\), then \(i \notin \setS_{2}\). \(\alpha_{\setS_{j}}\) denotes the vector of \(\alpha_{i}\)s such that \(i \in \setS_{j}\).

Now substituting \ref{beta-x-tilde}, \ref{stationarity-beta}, \ref{stationarity-beta-0} and \ref{stationarity-xi} in \ref{lagrangian}, we get:
\begin{multline}
\displaystyle \lagL = \frac{1}{2}\alpha^{T}\xtilde\xtilde^{T}\alpha + \sum_{i\in\setS_{1}} (C_{1} - \alpha_{i} - \mu_{i})\xi_{i} + \sum_{i\in\setS_{2}} (C_{2} - \alpha_{i} - \mu_{i})\xi_{i} + \\\sum_{i=1}^{n} \alpha_{i} - \sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_{i}\alpha_{j} y_{i}y_{j} x_{i}^{T}x_{j} - \beta_{0}\sum_{i=1}^{n}\alpha_{i}y_{i} \hspace{4mm} \text{ (Using \ref{beta-x-tilde} and \ref{stationarity-beta}) }
\end{multline}
\begin{gather}
\displaystyle \implies \lagL = \frac{1}{2}\alpha^{T}\xtilde\xtilde^{T}\alpha + \sum_{i=1}^{n} \alpha_{i} - ||\beta||_{2}^{2} \hspace{4mm} \text{ (Using \ref{stationarity-beta-0}, \ref{stationarity-beta} and \ref{stationarity-xi}) } \\
\label{dual-objective}
\displaystyle \implies \lagL = -\frac{1}{2}\alpha^{T}\xtilde\xtilde^{T}\alpha + \sum_{i=1}^{n} \alpha_{i} = -\frac{1}{2}\alpha^{T}\xtilde\xtilde\alpha + \alpha^{T}\mathbf{1} \hspace{4mm} \text{ (Using \ref{beta-x-tilde}) }
\end{gather}

Now since the primal problem \ref{primal} was a minimization problem, the dual will be a maximization problem. Using \ref{dual-objective}, \ref{dual-constraints}, we formulate the dual problem is below:
\begin{equation}
\label{dual}
\maximizewrt{\alpha \in \real^{n}} -\frac{1}{2}\alpha^{T}\xtilde\xtilde\alpha + \alpha^{T}\mathbf{1} \hspace{5mm}
\text{subject to } y^{T}\alpha = 0, \hspace{2mm} \mathbf{0} \leq \alpha_{\setS_{j}} \leq C_{j}\mathbf{1} \hspace{2mm} \forall j \in \{1, 2\}
\end{equation}
\end{flushleft}

\subsection*{Question 4}
\ref{primal} is a Quadratic Programming problem and a Second Order Cone Program, whereas the dual formulation \ref{dual} is a Quadratic program.

\section*{Part b}
\subsection*{Question 1}
\begin{flushleft}
Below is the table representing the objective value of the optimal solution.
\begin{center}
\begin{tabular}{|c|c|}
\hline
Penalty Parameter combination \((C_{1}, C_{2})\) & Objective Value \\
\hline
\hline
\((1, 1)\) & \(9.60436\)\\
\hline
\((1, 10)\) & \(30.26522\)\\
\hline
\((10, 1)\) & \(11.89106\)\\
\hline
\end{tabular}
\end{center}
\end{flushleft}

\subsection*{Question 2}
The reason of different decision boundaries occuring with variation in the penalty parameters could be attributed to the relative weights assigned to the classes. For example, if we set \(C_{1} = C_{2} = 1\), we are implicitly stating that both the classes \(\{+1, -1\}\) are equally important. On the other hand, if we assign \(C_{1} = 10, C_{2} = 1\), then we are implicitly stating that \(+1\) gets a higher priority over \(-1\), in the sense that we can't afford to mis-classify many of \(+1\) datapoints, but we could rather compromise with \(-1\) datapoints. An analogous case follows for \(C_{1} = 1, C_{2} = 10\), wherein we ``weigh'' \(-1\) more than \(+1\). The variation can be seen as the decision boundary tries to keep a higher number of examples to the class having a higher penalty on one side of itself.

\subsection*{Question 3}
\begin{itemize}
\item If \(\alpha_{i} = 0\), then the points \(x_{i}\) don't contribute to the boundary of the margin, since we know from \ref{stationarity-beta} that \(\beta\) is dependent on \(\alpha_{i}\)s. From observation of the graph generated, this also seems to mean that these refer to the points being correctly classified.
\item if \(\alpha_{i} \in (0, C_{k}), i \in \setS_{k}\), then the \(\mu_{i} > 0 \implies \xi_{i} = 0\) from \ref{compl-slack-mu}. This means that these are \(x_{i}\)s lying on the margins of the boundaries.
\item if \(\alpha_{i} = C_{k}, i \in \setS_{k}\), then \(\mu_{i} = 0 \implies \xi_{i} \geq 0\) from \ref{compl-slack-mu}. This means that these refer to points being incorrectly classified or points on the margin of the boundary.
\end{itemize}

\subsection*{Question 4}
The total classification error calculated in a zero-one loss manner is below:
\begin{center}
\begin{tabular}{|c|c|}
\hline
Penalty Parameter combination \((C_{1}, C_{2})\) & Weighted Classification Error \\
\hline
\hline
\((1, 1)\) & \(3\) \\
\hline
\((1, 10)\) & \(16\) \\
\hline
\((10, 1)\) & \(4\) \\
\hline
\end{tabular}
\end{center}

\end{document}
