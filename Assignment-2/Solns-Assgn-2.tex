\documentclass[11pt]{article}
\usepackage[margin=0.8in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bookmark}
\usepackage{hyperref}

\title{Solutions to the Assignment - 2 : CS6230 - \\
Optimization Methods in Machine Learning}
\author{Vishwak Srinivasan\\
\texttt{CS15BTECH11043}}
\date{}

\begin{document}
\maketitle

\section*{Question 1}
\subsection*{Part a}
\begin{flushleft}
\begin{equation}
\label{ques-1a}
\displaystyle f(x) = \max_{i=1\ldots m} (a_{i}^{T}x + b_{i})
\end{equation}

Note that \ref{ques-1a} is of the form: \(\displaystyle f(x) = \max_{i=1\ldots m} f_{i}(x)\). The steps involved in finding the subgradient of \(f\) at a given point are as follows:

\begin{enumerate}
\item See if \(f_{i}\) is subdifferentiable \(\forall i \in \{1 \ldots m\}\).
\item If there is no intersection, then the subdifferential of \(f\) is the subdifferential of the unique maximum.
\item If there is intersection, then the subdifferential of \(f\) at the point of intersection of is the set of all convex combinations of the subdifferentials of the function intersecting at that point.
\end{enumerate}

The first bullet point is simple to check: \(\nabla f_{i}(x) = a_{i}\). The second and third bullet point can be combined to form the following: the active functions at a given point are the set of functions such that \(f(x) = f_{\text{active}}(x)\). Note that in the second point, this set is singleton, whereas in the third point, this set maybe as big as \(m\). Hence the subdifferential of \(f\) is given by:
\begin{equation}
\label{ans-1a}
\partial f = \text{Conv}\left(\{\partial f_{i}(x) | f_{i}(x) = f(x) \text{ and } i \in \{1 \ldots m\}\}\right)
\end{equation}

where the \(\text{Conv}(S)\) in \ref{ans-1a} denotes the set of all convex combinations of the elements of \(S\)/convex hull of \(S\).
\end{flushleft}

\subsection*{Part b}
\begin{flushleft}
\begin{equation}
\label{ques-1b}
\displaystyle f(x) = \max_{i=1\ldots m} |a_{i}^{T}x + b_{i}|
\end{equation}

Note that \ref{ques-1b} takes the same form as \ref{ques-1a}. In fact, for \ref{ques-1b}, one can apply the results of \ref{ques-1a} twice. 
The steps are as follows:
\begin{enumerate}
\item See if \(f_{i}\) is subdifferentiable \(\forall i \in \{1 \ldots m\}\).
\item Get the subdifferential using the convex combination of the subdifferentials of every active function.
\end{enumerate}

For the first step, we know that \(\displaystyle f_{i} = \max \{-a_{i}^{T}x - b_{i}, a_{i}^Tx + b_{i}\}\). From the results of Question 1a and also the fact that there are more than one active functions at \(x = \mathbf{0}\), we can tell that:
\begin{equation}
\label{sub-ans-1b}
\partial f_{i}(0) = \text{Conv}(-a_{i}, a_{i}) = \{a_{i} - 2\theta a_{i} | \theta \in [0, 1]\}
\end{equation}

Using \ref{sub-ans-1b}, we replicate the same statement from \ref{ans-1a}, which is:
\begin{equation}
\label{ans-1b}
\partial f = \text{Conv}\left(\{\partial f_{i}(x) | f_{i}(x) = f(x) \text{ and } i \in \{1 \ldots m\}\}\right)
\end{equation}

where the \(\text{Conv}(S)\) in \ref{sub-ans-1b} and \ref{ans-1b} denotes the set of all convex combinations of the elements of \(S\)/convex hull of \(S\).
\end{flushleft}

\subsection*{Part c}
\begin{flushleft}
\begin{equation}
\label{ques-1c}
\displaystyle f(x) = \sup_{0 \leq t \leq 1} x_{1} + x_{2}t + \ldots + x_{n}t^{n-1}
\end{equation}

Since the set \([0, 1]\) over which the supremum is being taken over in \ref{ques-1c} is closed and bounded, meaning it is compact, following from the lecture notes\footnote{\href{https://see.stanford.edu/materials/lsocoee364b/01-subgradients_notes.pdf}{Boyd and Vanderberghe Lecture Notes on Subgradients}}, we can apply the same intuition as done for \ref{ques-1a} and \ref{ques-1b}. In this case, it is required that \(p(t)\) is sub-differentiable w.r.t. \(x\). This is assured since the gradient is:
\begin{equation}
\label{sub-ans-1c}
\displaystyle \nabla_{x} p(t) =  \langle 1, t, \ldots, t^{n-1} \rangle
\end{equation}

Using \ref{sub-ans-1c}, we use the same statement as in \ref{ans-1a} and \ref{ans-1b},
\begin{equation}
\label{ans-1c}
\displaystyle \partial f = \text{Conv}\left(\{\nabla_{x} p(t) | f(x) = \sup p(t), t \in [0, 1]\}\right)
\end{equation}

where the \(\text{Conv}(S)\) in \ref{ans-1c} denotes the set of all convex combinations of the elements of \(S\)/convex hull of \(S\).
\end{flushleft}

\subsection*{Part d}
\begin{flushleft}
\begin{equation}
\label{ques-1d}
\displaystyle f(x) = x_{[1]} + x_{[2]} + \ldots + x_{[k]}
\end{equation}

Clearly, those indices not involved in the first \(k\) highest values will have their partial derivatives to be \(0\) and those indices involved in the first \(k\) highest values will have their partial derivatives to be \(1\). 
\(\newline\)

To get these indices, we need to maintain a copy of the vector and sort the elements of that copy. Note the indices involved in the top \(k\) values. Call the set to be \(S\). Hence the subgradient for \ref{ques-1d} would be:
\begin{equation}
\label{ans-1d}
\displaystyle g \text{ such that } g_{i} = \begin{cases} 1 & \text{ if } i \in S \\ 0 & \text{ otherwise} \end{cases}
\end{equation}

Note that \ref{ans-1d} will vary based on the value of \(k\) too. 
\end{flushleft}
\newpage

\section*{Question 2}
\begin{flushleft}
To prove convexity, consider 2 points: \(x\) and \(y\).
\subsubsection*{Case 1}
Let \(x = 0\) and \(y = 0\). Because of this, any convex combination of \(x\) and \(y\) has to be \(0\). Formally, \(\forall \hspace{1.5mm} \theta \in [0, 1]\),
\begin{gather*}
f(\theta x + (1 - \theta) y) = f(0) = 1 \\
\theta f(x) + (1 - \theta) f(y) = \theta + 1 - \theta = 1
\end{gather*}
Since \(f(\theta x + (1 - \theta) y) = \theta f(x) + (1 - \theta) f(y)\), convexity holds.

\subsubsection*{Case 2}
Without loss of generality, let \(x > 0\) and \(y = 0\). \(\forall \hspace{1.5mm} \theta \in [0, 1]\),
\begin{gather*}
f(\theta x + (1 - \theta)y) = f(\theta x) = \begin{cases} 1 & \text{if} \theta = 0 \\ 0 & \text{otherwise} \end{cases} \\
\theta f(x) + (1 - \theta) f(y) = (1 - \theta)
\end{gather*}
If \(\theta = 0\), then \[f(\theta x + (1 - \theta)y) = \theta f(x) + (1 - \theta) f(y)\] Else, \[f(\theta x + (1 - \theta)y) = 0 \leq \theta f(x) + (1 - \theta) f(y) = (1  - \theta)\] Convexity holds.

\subsubsection*{Case 3}
Let \(x > 0\) and \(y > 0\). Because of this, any convex combination of \(x\) and \(y\) has to be \(> 0\). Formally, \(\forall \hspace{1.5mm} \theta \in [0, 1]\),
\begin{gather*}
f(\theta x + (1 - \theta)y) = 0 \\
\theta f(x) + (1 - \theta) f(y) = 0
\end{gather*}
Since \(f(\theta x + (1 - \theta) y) = \theta f(x) + (1 - \theta) f(y)\), convexity holds.

Since in all cases, convexity held, the function is verified to be convex.

Consider the task of finding the subgradients at \(x = 0\). By the definition of subgradients:
\begin{gather*}
f(y) \geq f(x) + g^{T}(y - x) \\
f(y) \geq 1 + gy
\end{gather*}

Now if \(y = 0\), then \(g\) could be any number. Otherwise if \(y > 0\), then we get this condition: \(g \leq \frac{-1}{y}\). Note that \(g\) should hold for all \(y\). Hence for the smallest \(y\) close to \(0\), then set of subgradients may be an empty set, hence leading the intersection to be an empty set. Since the set of subgradients is empty, the subdifferentials at \(x = 0\) are empty, leading to conclude that \(f\) is not subdifferentiable at \(x = 0\).
\end{flushleft}

\section*{Question 3}
\subsection*{Part a}
\begin{flushleft}
Consider the SVD of \(W\) to be \(W = U_{W} \Sigma_{W} V^{T}_{W}\). From the conditions imposed on \(W\), we get: 

Hence the SVD of \(UV^{T} + W\) can be given by:
\begin{equation}
\label{svd-uv-w}
UV^{T} + W = U I V^{T} + U_{W} \Sigma_{W} V^{T}_{W} = \begin{bmatrix} U & U_{W} \end{bmatrix} \begin{bmatrix} I & 0 \\ 0 & \Sigma_{W} \end{bmatrix} \begin{bmatrix} V^{T} \\ V_{W}^{T} \end{bmatrix}
\end{equation}

This is a valid SVD because:
\begin{equation*}
U^{T}W = 0 \implies U^{T}U_{W}\Sigma_{W}V^{T}_{W} = 0 \implies U^{T}U_{W}\Sigma_{W}V^{T}_{W}V_{W} = 0 \implies U^{T}U_{W}\Sigma_{W} = 0 \implies U^{T}U_{W} = 0
\end{equation*}

Similarly it can be shown that \(VV_{W}^{T} = 0\) (by using \(WU = 0\) and pre-multiplying \(U^{T}_{W}\)). From these results, it can be shown that:
\begin{gather*}
\begin{bmatrix} U & U_{W} \end{bmatrix}^{T} \begin{bmatrix} U & U_{W} \end{bmatrix} = I_{2r} \\
\begin{bmatrix} V & V_{W} \end{bmatrix}^{T} \begin{bmatrix} V & V_{W} \end{bmatrix} = I_{2r}
\end{gather*}

Since \(||W||_{\text{op}} \leq 1\), we can tell that the diagonal entries of \(\Sigma_{W}\) are \(\leq 1\). Hence the maximum diagonal element in the singular matrix for \(UV^{T} + W\) from \ref{svd-uv-w} is \(1\), which suggests that \(||UV^{T} + W||_{\text{op}} \leq 1\)
\end{flushleft}

\subsection*{Part b}
\begin{flushleft}
Using the SVD of \(A\), we have:
\begin{equation}
\label{uv-w-a-prod}
(UV^{T} + W)^{T}A = (VU^{T} + W^{T})(U\Sigma V^{T}) = VU^{T}U\Sigma V^{T} + W^{T}U\Sigma V^{T} = V\Sigma V^{T}
\end{equation}

Now, using the properties of circularity of the trace and \ref{uv-w-a-prod}, we get:
\begin{equation}
\label{part-3b}
\text{tr}((UV^{T} + W)^{T}A) = \text{tr}(V\Sigma V^{T}) = \text{tr}(V^{T}V\Sigma) = \text{tr}(\Sigma) = ||A||_{*}
\end{equation}
\end{flushleft}

\subsection*{Part c}
\begin{flushleft}
We know that the dual norm of the operator norm is the trace norm. Hence by the definition of trace norm:
\begin{equation}
\label{trace-norm-dual}
\displaystyle ||A||_{*} = \max_{||B||_{\text{op}} \leq 1} \text{tr}(B^{T} A)
\end{equation}
From the result in \ref{ans-1a}, \ref{trace-norm-dual} and \(\nabla_{A} \text{tr}(B^{T}A) = B\), we know how to obtain the subdifferential of \(||A||_{*}\). This happens to be:
\begin{equation}
\label{subdiff-trace}
\displaystyle \partial ||A||_{*} = \text{Conv}\left(\{B \ni \text{tr}(B^{T}A) = ||A||_{*}, ||B||_{\text{op}} \leq 1\}\right)
\end{equation}
From the results in \ref{part-3b} and \ref{svd-uv-w}, we can see that \(UV^{T} + W\) for any \(W\) satisfying the same condition as the elements of \(\partial ||A||_{*}\). Hence we can conclude that:
\begin{multline}
\label{subset-rel}
\text{Conv}\left(\{B \ni \text{tr}(B^{T}A) = ||A||_{*}, ||B||_{\text{op}} \leq 1\}\right) \\ \supseteq \{UV^{T} + W \ni ||W||_{\text{op}} \leq 1, U^{T}W = 0, WV = 0, \text{tr}((UV^{T} + W)^{T}A) = ||A||_{*}\}
\end{multline}

Using \ref{subset-rel} and \ref{subdiff-trace}, we get the final result:
\begin{equation}
\label{ans-3c}
\displaystyle \partial ||A||_{*} \supseteq \{UV^{T} + W \ni ||W||_{\text{op}} \leq 1, U^{T}W = 0, WV = 0, \text{tr}((UV^{T} + W)^{T}A) = ||A||_{*} \}
\end{equation}

\end{flushleft}

\section*{Question 4}
\subsection*{Part a}
\begin{flushleft}
We know that if \(\nabla g\) is L-Lipschitz, then:
\begin{equation}
\label{grad-lips}
\nabla^2 g(x) \preceq LI
\end{equation}
where \(I\) is the identity matrix of corresponding dimensions.
\(\newline\)

Hence by using a \(2^{nd}\) order Taylor series expansion of \(g\) around a given \(x\), we get:
\begin{equation}
\label{second-taylor}
\displaystyle g(y) \approx g(x) + \nabla g(x)^{T}(y - x) + \frac{(y - x)^{T}\nabla^2 g(x) (y - x)}{2}
\end{equation}
Note that we have assumed that \(g\) is twice-differentiable. The proof breaks down if this is not taken into consideration.
\(\newline\)

Using \ref{grad-lips} in \ref{second-taylor}, we get:
\begin{equation}
\label{grad-lips-taylor}
\displaystyle g(y) \leq g(x) + \nabla g(x)^{T}(y - x) + \frac{L}{2}||y - x||_{2}^{2}
\end{equation}

Since \(f(y) = g(y) + h(y)\), substituting \ref{grad-lips-taylor}:
\begin{equation}
\label{part-a}
f(y) \leq g(x) + \nabla g(x)^{T}(y - x) + \frac{L}{2}||y - x||_{2}^{2} + h(y)
\end{equation}
\end{flushleft}

\subsection*{Part b}
\begin{flushleft}
Let \(y = x^{+} = x - tG_{t}(x)\). Substituting this in \ref{part-a}, we get:
\begin{equation}
\label{update-subs}
f(x^{+}) \leq g(x) + \nabla g(x)^{T}(-tG_{t}(x)) + \frac{Lt^{2}}{2}||G_{t}(x)||_{2}^{2} + h\left(x - tG_{t}(x)\right)
\end{equation}

Since \(t \leq \frac{1}{L}\), \(L \leq \frac{1}{t}\) and consequently \(\frac{Lt^2}{2} \leq \frac{t}{2}\). Substitute this is \ref{update-subs} to get:
\begin{equation}
\label{part-b}
f(x^{+}) \leq g(x) - t\nabla g(x)^TG_{t}(x) + \frac{t}{2}||G_{t}(x)||_{2}^{2} + h\left(x - tG_{t}(x)\right)
\end{equation}
\end{flushleft}

\subsection*{Part c}
\begin{flushleft}
By subgradient optimality condition:
\begin{equation}
\label{subgrad-optim}
0 \in \partial\left(\frac{||z-x||^2}{2t} + h(z)\right)  \implies \frac{x - z}{t} \in \partial h(z) \implies \frac{x - \textbf{prox}_{h, t}(x)}{t} \in \partial h(\textbf{prox}_{h, t}(x))
\end{equation}

Substitute \(x \leftarrow x - t\nabla g(x)\) in \ref{subgrad-optim} to get:
\begin{equation}
\label{update-subgrad-optim}
\frac{x - t\nabla g(x) - \textbf{prox}_{h, t}(x - t\nabla g(x))}{t} \in \partial h(\textbf{prox}_{h, t}(x - t\nabla g(x)))
\end{equation}

Using the definition of \(\textbf{prox}_{h, t}(x - t\nabla g(x)) = x - tG_{t}(x)\) in \ref{update-subgrad-optim}, we get:
\begin{equation}
\label{sub-part-c}
\frac{x - t\nabla g(x) - x + tG_{t}(x)}{t} \in \partial h(x - tG_{t}(x)) \implies \frac{tG_{t}(x) - t\nabla g(x)}{t} \in \partial h(x - tG_{t}(x)) 
\end{equation}

Using \ref{sub-part-c}, we have:
\begin{equation}
\label{part-c}
\implies G_{t}(x) - \nabla g(x) \in \partial h(x - tG_{t}(x))
% TODO: Need to check this equation
\end{equation}
\end{flushleft}

\subsection*{Part d}
\begin{flushleft}
Note that the required inequation has a \(f(z)\) in the RHS, indicating the presence of \(g(z)\) and \(h(z)\) in our steps.
Using the definition of convexity of \(g\) and \(h\) we get:
\begin{equation}
\label{cvx-g}
g(z) \geq g(x) + \nabla g(x)^{T}(z - x) \implies g(x) \leq g(z) - \nabla g(x)^{T}(z - x)
\end{equation}
\begin{equation}
\label{cvx-h}
h(z) \geq h(x) + s(x)^{T}(z - x) \implies h(x) \leq h(z) - s(x)^{T}(z - x)
\end{equation}

Substituting \(x \leftarrow x - tG_{t}(x)\) in \ref{cvx-h} and using the result in \ref{part-c}, we get:
\begin{equation}
\label{update-cvx-h}
h(x - tG_{t}(x)) \leq h(z) - s(x - tG_{t}(x))^{T}(z - x) = h(z) - (G_{t}(x) - \nabla g(x))^{T}(z - x + tG_{t}(x))
\end{equation}

Using \ref{part-b} and the substituting the results from \ref{cvx-g} and \ref{update-cvx-h}:
\begin{equation}
\label{step-d1}
f(x^{+}) \leq g(z) - \nabla g(x)^{T}(z - x) - t\nabla g(x)^{T}G_{t}(x) + \frac{t}{2}||G_{t}(x)||_{2}^{2} + h(z) - (G_{t}(x) - \nabla g(x))^{T}(z - x + tG_{t}(x))
\end{equation}

Performing the rearrangement: \[(G_{t}(x) - \nabla g(x))^{T}(z - x + tG_{t}(x)) = (G_{t}(x) - \nabla g(x))^{T}(z - x) + t||G_{t}(x)||_{2}^{2} - t\nabla g(x)^{T}G_{t}(x)\] and substituting in \ref{step-d1} we proceed to get:
\begin{equation}
\label{step-d2}
f(x^{+}) \leq g(z) - (\nabla g(x) + G_{t}(x) - \nabla g(x))^{T}(z - x) - t\nabla g(x)^{T}G_{t}(x) + t\nabla g(x)^{T}G_{t}(x) - \frac{t}{2}||G_{t}(x)||_{2}^{2}
\end{equation}

Cancelling out the required terms in \ref{step-d2} and using the fact that \(f(z) = g(z) + h(z)\)
\begin{equation}
\label{part-d}
f(x^{+}) \leq f(z) - G_{t}(x)^{T}(z - x) - \frac{t}{2}||G_{t}(x)||_{2}^{2} = f(z) + G_{t}(x)^{T}(x - z) - \frac{t}{2}||G_{t}(x)||_{2}^{2}
\end{equation}

\subsection*{Part e}
We know that \(x^{+}\) is the updated after \(G_{t}(x)\) is applied to \(x\). Substituting \(z = x\) in \ref{part-d}, we can see that:
\begin{equation}
\label{guarantee}
f(x^{+}) \leq f(x) - \frac{t}{2}||G_{t}(x)||_{2}^{2}
\end{equation}

Since \(||.||\) is positive, \ref{guarantee} guarantees a decrease in the function value after a step is made. Use the update rule \(x^{+} = x - tG_{t}(x)\) to get \(G_{t}(x) = \frac{x - x^{+}}{t}\). Substituting: \(z = x^{*}\) and the equation for \(G_{t}(x)\) in \ref{part-d}, we get:
\begin{equation}
\label{step-e1}
f(x^{+}) \leq f(x^{*}) + \frac{1}{2t}\left((x - x^{+})^{T}(2x - 2x^{*}) - (x - x^{+})^{T}(x - x^{+})\right)
\end{equation}

Playing with the second term:
\begin{gather*}
2x - 2x^{*} - x + x^{+} = (x - x^{*}) - (x^{*} - x^{+}) \\
\implies (x - x^{+})^{T}(2x - 2x^{*} - (x - x^{+})) = (x - x^{+})^{T}(x - x^{*}) - (x - x^{+})^{T}(x^{*} - x^{+})
\end{gather*}

Splitting each \(x - x^{+}\) as \((x - x^{*}) + (x^{*} - x^{+})\):
\begin{gather*}
(x - x^{+})^{T}(x - x^{*}) = ||x - x^{*}||_{2}^{2} + (x^{*} - x^{+})^{T}(x - x^{*}) \\
(x - x^{+})^{T}(x^{*} - x^{+}) = (x - x^{*})^{T}(x^{*} - x^{+}) + ||x^{*} - x^{+}||_{2}^{2} \\
\implies (x - x^{+})^{T}(x - x^{*}) - (x - x^{+})^{T}(x^{*} - x^{+}) = ||x - x^{*}||_{2}^{2} - ||x^{+} - x^{*}||_{2}^{2} \\
\implies (x - x^{+})^{T}(2x - 2x^{*} - (x - x^{+})) = ||x - x^{*}||_{2}^{2} - ||x^{+} - x^{*}||_{2}^{2}
\end{gather*}

Substituting the final results back in \ref{step-e1}, we get:
\begin{equation}
\label{part-e}
f(x^{+}) \leq f(x^{*}) + \frac{1}{2t}\left(||x - x^{*}||_{2}^{2} - ||x^{+} - x^{*}||_{2}^{2}\right)
\end{equation}
\end{flushleft}

\subsection*{Part f}
\begin{flushleft}
We know from \ref{guarantee} that \(f(x^{(k)}) \leq f(x^{(k-1)})\). Substitute \(x^{+} \leftarrow x^{(k)}\) and \(x \leftarrow x^{(k-1)}\) in \ref{part-e}. We now get:
\begin{equation}
\label{step-f1}
f(x^{(k)}) - f(x^{*}) \leq \frac{1}{2t}\left(||x^{(k-1)} - x^{*}||_{2}^{2} - ||x^{(k)} - x^{*}||_2^{2}\right)
\end{equation}

Perform a summation over \(k\) running from 1 to \(K\) on both sides of \ref{step-f1}, which yields us:
\begin{equation}
\label{step-f2}
\displaystyle \sum_{k=1}^{K} f(x^{(k)}) - f(x^{*}) \leq \frac{1}{2t} \left(\sum_{k=1}^{K} ||x^{(k-1)} - x^{*}||_{2}^{2} - ||x^{(k)} - x^{*}||_2^{2}\right)
\end{equation}

The sum on the RHS of \ref{step-f2} is conveniently a telescoping sum.
\begin{equation}
\label{step-f3}
\displaystyle \sum_{k=1}^{K} f(x^{(k)}) - f(x^{*}) \leq \frac{1}{2t} \left(||x^{0} - x^{*}||_{2}^{2} - ||x^{K} - x^{*}||_{2}^{2}\right)
\end{equation}

Since \(f(x^{(K)}) \leq f(x^{(K-1)}) \leq \ldots \leq f(x^{(1)})\), we can lower-bound the value \(f(x^{(k)}) - f(x^{*})\) by \(f(x^{(K)}) - f(x^{*})\). Also \(||x^{(0)} - x^{*}||_{2}^{2} - ||x^{(K)} - x^{*}||_{2}^{2} \leq ||x^{(0)} - x^{*}||_{2}^{2}\). Using these substitutions in \ref{step-f3}, we get:
\begin{equation}
\label{part-f}
f(x^{(K)}) - f(x^{*}) \leq \frac{||x^{(0)} - x^{*}||_{2}^{2}}{2tK}
\end{equation}
\end{flushleft}
\end{document}
